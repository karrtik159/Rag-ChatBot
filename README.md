# ğŸ¤– RAG Chatbot â€“ Document Embedding & Query API

A Retrievalâ€‘Augmentedâ€‘Generation stack that:

* ingests PDF / DOCX / TXT (incl. OCR)
* chunks + embeds with **BGEâ€‘smallâ€‘en v1.5**
* stores vectors in **Qdrant**
* reranks, then answers with **GPTâ€‘4â€¯/â€¯any OpenAIâ€‘compatible model** (local or cloud)
* returns tight, cited answers with conversation memory

Everything can run **entirely locally** via Docker; no data ever leaves your machine.

---

## ğŸ—ºï¸ Project Layout

```
.                   # application code
â”œâ”€â”€ rag_chatbot/
â”‚   â”œâ”€â”€ main.py          # FastAPI entryâ€‘point
â”‚   â”œâ”€â”€ models.py        # envâ€‘driven settings & schemas
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ ingest_service.py
â”‚   â”‚   â””â”€â”€ chatbot_manager.py
â”‚   â”œâ”€â”€ parsers/       # pdf_parser.py, docx_parser.py, txt_parser.py
â”‚   â”œâ”€â”€ chunker.py
â”‚   â”œâ”€â”€ embedder.py
â”‚   â””â”€â”€ storage/
â”‚       â””â”€â”€ qdrant_client.py
â”œâ”€â”€ pyproject.toml       # Poetry config
â”œâ”€â”€ poetry.lock
â”œâ”€â”€ Dockerfile           # multiâ€‘stage build
â”œâ”€â”€ docker-compose.yml   # app + Qdrant
â””â”€â”€ .env                 # *never* commit real keys
```

---

## âš™ï¸ Prerequisites (local run)

| Tool              | Purpose            | Install                                                  |
| ----------------- | ------------------ | -------------------------------------------------------- |
| **Python 3.11**   | runtime            | [https://python.org](https://python.org)                 |
| **Poetry 1.8+**   | dependency manager | `pip install poetry`                                     |
| **Tesseractâ€‘OCR** | scannedâ€‘PDF text   | `sudo apt install tesseract-ocr` / Windows installer     |
| **Qdrant**        | vector DB          | `docker run -p 6333:6333 qdrant/qdrant` (or via compose) |

> **Optional:** GPUâ€‘ready PyTorch, Ollama, etc. if you want fully offline LLMs.

---

## ğŸ”‘Â EnvironmentÂ Vars (`.env`)

```dotenv
# Qdrant
QDRANT_URL=http://qdrant:6333
QDRANT_API_KEY=

# OpenAI (or any compatible gateway)
OPENAI_API_KEY=
OPENAI_API_BASE_URL=https://api.openai.com/v1   # point to local server if needed

# Embeddings
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5

# HuggingFace token (only if you flip USE_HF_INFERENCE_API=true)
HF_TOKEN=
```

Create `.env` (or copy `.env.example`) before running.

---

## ğŸÂ Running with Poetry

```bash
# 1. Install deps (prod only)
poetry install --only main

# 2. Activate shell
poetry shell

# 3. Launch dev server
uvicorn rag_chatbot.main:app --reload --port 8000
```

Interactive docs: `http://localhost:8000/docs`

---

## ğŸ³Â Running with DockerÂ (+Â Compose)

### 1â€¯/â€¯Standalone image (quick)

```bash
docker build -t rag-chatbot .
docker run -p 8000:8000 -p 6333:6333 --env-file .env rag-chatbot
```

*Image includes Qdrant inside the same container for zeroâ€‘config demos.*

### 2â€¯/â€¯Compose (recommended dev)

```bash
docker compose --env-file .env up --build
```

* `app` âœ [http://localhost:8000/docs](http://localhost:8000/docs)
* `qdrant` âœ [http://localhost:6333](http://localhost:6333) (REST & gRPC)

Stop with `Ctrlâ€‘C`; data lives in the `qdrant_data` volume.

---

## ğŸš€Â Key Endpoints

| Method | Path      | Body                             | Description                                             |
| ------ | --------- | -------------------------------- | ------------------------------------------------------- |
| POST   | `/ingest` | `multipart/form-data`Â (file)     | Embeds a document â†’ returns `document_id` & chunk count |
| POST   | `/query`  | `{ "query": "...", "top_k": 3 }` | Returns answer + citations                              |
| GET    | `/health` | â€“                                | Simple liveness probe                                   |

Full SwaggerÂ /Â ReDoc at `/docs`Â & `/redoc`.

---

## ğŸ“ˆÂ Performance (M1Â MBA, 8â€‘coreÂ CPU)

| Stage            | PDFâ€¯(10â€¯pages) | Notes                            |
| ---------------- | -------------- | -------------------------------- |
| IngestionÂ +Â OCR  | 2.3â€¯s          | pdfplumber + Tesseract           |
| Embedding Upsert | 1.1â€¯s          | batched BGE (CPU)                |
| Query (topâ€‘3)    | 650â€¯ms         | includes rerank + GPTâ€‘4 response |

Switch to GPU or locallyâ€‘quantised LLM to speed things up / cut costs.

---

## ğŸ”Â SecurityÂ &Â Privacy

* All docs & vectors stay on your disk.
* LLM calls default to **OpenAI**; point `OPENAI_API_BASE_URL` to a local llamaâ€‘cpp or LM Studio endpoint for 100â€¯% offline.
* HTTPS, auth headers, and rate limiting can be added via a reverse proxy (Traefik / Nginx).

---

## ğŸ“„Â License

MIT â€” use, modify, and distribute as you please. Pull requests welcome!
